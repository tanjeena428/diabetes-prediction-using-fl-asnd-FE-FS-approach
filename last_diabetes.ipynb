{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlg141TgJG05",
        "outputId": "77defb01-f452-49dc-a460-56802c8bf98e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "Top 10 Features - ANOVA:\n",
            "Gender: 5.2086\n",
            "Polyuria: 130.9688\n",
            "Polydipsia: 412.7384\n",
            "sudden weight loss: 376.4226\n",
            "weakness: 121.9737\n",
            "Polyphagia: 32.5851\n",
            "visual blurring: 68.8418\n",
            "Irritability: 6.3782\n",
            "partial paresis: 34.9178\n",
            "Alopecia: 0.0928\n",
            "\n",
            "Top 10 Features - Chi-squared:\n",
            "Gender: 0.2110\n",
            "Polyuria: 38.7476\n",
            "Polydipsia: 116.1846\n",
            "sudden weight loss: 120.7855\n",
            "weakness: 57.7493\n",
            "Polyphagia: 12.7243\n",
            "visual blurring: 33.1984\n",
            "Irritability: 4.9140\n",
            "partial paresis: 18.1246\n",
            "Alopecia: 0.0478\n",
            "\n",
            "Top 10 Features - Correlation:\n",
            "Polyuria: 0.6659\n",
            "Polydipsia: 0.6487\n",
            "Gender: 0.4492\n",
            "sudden weight loss: 0.4366\n",
            "partial paresis: 0.4323\n",
            "Polyphagia: 0.3425\n",
            "Irritability: 0.2995\n",
            "Alopecia: 0.2675\n",
            "visual blurring: 0.2513\n",
            "weakness: 0.2433\n",
            "\n",
            "Running FS: CHI2 + FE: PCA\n",
            "Completed FS: CHI2, FE: PCA\n",
            "\n",
            "Running FS: CHI2 + FE: LDA\n",
            "Completed FS: CHI2, FE: LDA\n",
            "\n",
            "Running FS: ANOVA + FE: PCA\n",
            "Completed FS: ANOVA, FE: PCA\n",
            "\n",
            "Running FS: ANOVA + FE: LDA\n",
            "Completed FS: ANOVA, FE: LDA\n",
            "\n",
            "Running FS: CORRELATION + FE: PCA\n",
            "Completed FS: CORRELATION, FE: PCA\n",
            "\n",
            "Running FS: CORRELATION + FE: LDA\n",
            "Completed FS: CORRELATION, FE: LDA\n",
            "\n",
            "Summary Results Table:\n",
            "         FS  FE  Accuracy  Precision  Recall  F1 Score\n",
            "       CHI2 PCA    0.8190     0.8243  0.9104    0.8652\n",
            "       CHI2 LDA    0.8952     0.8861  0.9722    0.9272\n",
            "      ANOVA PCA    0.8571     0.8615  0.9032    0.8819\n",
            "      ANOVA LDA    0.8476     0.8219  0.9524    0.8824\n",
            "CORRELATION PCA    0.7714     0.7250  0.9667    0.8286\n",
            "CORRELATION LDA    0.9238     0.9322  0.9322    0.9322\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Cell 1: Mount Google Drive and load the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_path = '/content/drive/MyDrive/fl1/diabetes.csv'\n",
        "import pandas as pd\n",
        "df = pd.read_csv(data_path)\n",
        "df.head()\n",
        "\n",
        "# Cell 2: Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Cell 3: Preprocess categorical columns\n",
        "# Convert categorical columns to numerical\n",
        "df['Gender'] = df['Gender'].map({'Male': 1, 'Female': 0})\n",
        "binary_columns = ['Polyuria', 'Polydipsia', 'sudden weight loss', 'weakness', 'Polyphagia',\n",
        "                  'Genital thrush', 'visual blurring', 'Itching', 'Irritability',\n",
        "                  'delayed healing', 'partial paresis', 'muscle stiffness', 'Alopecia', 'Obesity']\n",
        "for col in binary_columns:\n",
        "    df[col] = df[col].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "# Convert target variable 'class' to numerical\n",
        "df['class'] = df['class'].map({'Positive': 1, 'Negative': 0})\n",
        "\n",
        "# Check for missing or infinite values\n",
        "if df.isnull().sum().sum() > 0 or np.isinf(df).sum().sum() > 0:\n",
        "    print(\"Warning: Dataset contains missing or infinite values. Handling them...\")\n",
        "    df = df.fillna(df.mean())  # Simple imputation for numerical columns\n",
        "    df = df.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "\n",
        "df.head()\n",
        "\n",
        "# Cell 4: Define feature and target variables\n",
        "X = df.drop(columns='class').values  # Features\n",
        "Y = df['class'].values  # Target\n",
        "feature_names = df.drop(columns='class').columns\n",
        "\n",
        "# Standardize numerical features (e.g., Age)\n",
        "scaler = StandardScaler()\n",
        "X[:, 0] = scaler.fit_transform(X[:, 0].reshape(-1, 1)).flatten()  # Standardize Age column\n",
        "\n",
        "# Cell 5: Feature Selection Functions\n",
        "def select_features(X, y, method=\"anova\", k=10, feature_names=None):\n",
        "    if method == \"chi2\":\n",
        "        # Ensure non-negative values for chi2\n",
        "        scaler = MinMaxScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "        selector = SelectKBest(score_func=chi2, k=k)\n",
        "        X_new = selector.fit_transform(X_scaled, y)\n",
        "        indices = selector.get_support(indices=True)\n",
        "    elif method == \"anova\":\n",
        "        selector = SelectKBest(score_func=f_classif, k=k)\n",
        "        X_new = selector.fit_transform(X, y)\n",
        "        indices = selector.get_support(indices=True)\n",
        "    elif method == \"correlation\":\n",
        "        # Correlation-based feature selection\n",
        "        df_temp = pd.DataFrame(X, columns=feature_names)\n",
        "        df_temp['target'] = y\n",
        "        # Compute absolute correlation with target\n",
        "        corr_with_target = df_temp.corr()['target'].abs().drop('target')\n",
        "        # Sort by correlation and select top k\n",
        "        top_features = corr_with_target.sort_values(ascending=False).head(k).index\n",
        "        # Check for multicollinearity (remove one of highly correlated pairs)\n",
        "        corr_matrix = df_temp[top_features].corr().abs()\n",
        "        to_drop = set()\n",
        "        for i in range(len(top_features)):\n",
        "            for j in range(i + 1, len(top_features)):\n",
        "                if corr_matrix.iloc[i, j] > 0.7:  # Threshold for multicollinearity\n",
        "                    # Drop the feature with lower correlation to target\n",
        "                    if corr_with_target[top_features[i]] > corr_with_target[top_features[j]]:\n",
        "                        to_drop.add(top_features[j])\n",
        "                    else:\n",
        "                        to_drop.add(top_features[i])\n",
        "        selected_features = [f for f in top_features if f not in to_drop][:k]\n",
        "        indices = [list(feature_names).index(f) for f in selected_features]\n",
        "        X_new = X[:, indices]\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported FS method\")\n",
        "    return X_new, indices\n",
        "\n",
        "# Cell 6: Feature Selection with Scores\n",
        "def get_feature_scores(X, y, method=\"anova\", k=10, feature_names=None):\n",
        "    if method == \"chi2\":\n",
        "        scaler = MinMaxScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "        selector = SelectKBest(score_func=chi2, k=k)\n",
        "        selector.fit(X_scaled, y)\n",
        "        scores = selector.scores_\n",
        "        indices = selector.get_support(indices=True)\n",
        "    elif method == \"anova\":\n",
        "        selector = SelectKBest(score_func=f_classif, k=k)\n",
        "        selector.fit(X, y)\n",
        "        scores = selector.scores_\n",
        "        indices = selector.get_support(indices=True)\n",
        "    elif method == \"correlation\":\n",
        "        df_temp = pd.DataFrame(X, columns=feature_names)\n",
        "        df_temp['target'] = y\n",
        "        corr_with_target = df_temp.corr()['target'].abs().drop('target')\n",
        "        top_features = corr_with_target.sort_values(ascending=False).head(k).index\n",
        "        corr_matrix = df_temp[top_features].corr().abs()\n",
        "        to_drop = set()\n",
        "        for i in range(len(top_features)):\n",
        "            for j in range(i + 1, len(top_features)):\n",
        "                if corr_matrix.iloc[i, j] > 0.7:\n",
        "                    if corr_with_target[top_features[i]] > corr_with_target[top_features[j]]:\n",
        "                        to_drop.add(top_features[j])\n",
        "                    else:\n",
        "                        to_drop.add(top_features[i])\n",
        "        selected_features = [f for f in top_features if f not in to_drop][:k]\n",
        "        indices = [list(feature_names).index(f) for f in selected_features]\n",
        "        scores = [corr_with_target[f] for f in selected_features]\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported method\")\n",
        "    return indices, scores\n",
        "\n",
        "# Get top 10 features for ANOVA, Chi-squared, and Correlation\n",
        "k = 10\n",
        "anova_indices, anova_scores = get_feature_scores(X, Y, method=\"anova\", k=k, feature_names=feature_names)\n",
        "chi2_indices, chi2_scores = get_feature_scores(X, Y, method=\"chi2\", k=k, feature_names=feature_names)\n",
        "corr_indices, corr_scores = get_feature_scores(X, Y, method=\"correlation\", k=k, feature_names=feature_names)\n",
        "\n",
        "# Get selected feature names and scores\n",
        "anova_features = [(feature_names[i], score) for i, score in zip(anova_indices, anova_scores)]\n",
        "chi2_features = [(feature_names[i], score) for i, score in zip(chi2_indices, chi2_scores)]\n",
        "corr_features = [(feature_names[i], score) for i, score in zip(corr_indices, corr_scores)]\n",
        "\n",
        "# Print selected features with scores\n",
        "print(\"\\nTop 10 Features - ANOVA:\")\n",
        "for feature, score in anova_features:\n",
        "    print(f\"{feature}: {score:.4f}\")\n",
        "\n",
        "print(\"\\nTop 10 Features - Chi-squared:\")\n",
        "for feature, score in chi2_features:\n",
        "    print(f\"{feature}: {score:.4f}\")\n",
        "\n",
        "print(\"\\nTop 10 Features - Correlation:\")\n",
        "for feature, score in corr_features:\n",
        "    print(f\"{feature}: {score:.4f}\")\n",
        "\n",
        "\n",
        "# Cell 7: Feature Extraction Function\n",
        "def extract_features(X, y, method=\"lda\", n_components=1):\n",
        "    if method == \"pca\":\n",
        "        extractor = PCA(n_components=n_components)\n",
        "        X_new = extractor.fit_transform(X)\n",
        "    elif method == \"lda\":\n",
        "        n_components = min(n_components, len(np.unique(y))-1)  # Ensure valid n_components for LDA\n",
        "        extractor = LDA(n_components=n_components)\n",
        "        X_new = extractor.fit_transform(X, y)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported FE method\")\n",
        "    return X_new\n",
        "\n",
        "# Cell 8: MLP Model Definition\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 100)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(100, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))\n",
        "\n",
        "# Cell 9: Training Function\n",
        "def train(model, loader, criterion, optimizer, epochs=20):\n",
        "    model.train()\n",
        "    for _ in range(epochs):\n",
        "        for X_batch, y_batch in loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(X_batch)\n",
        "            loss = criterion(output, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "# Cell 10: Model Aggregation Function\n",
        "def average_models(models):\n",
        "    new_model = MLP(models[0].fc1.in_features)\n",
        "    new_state_dict = new_model.state_dict()\n",
        "    for key in new_state_dict:\n",
        "        new_state_dict[key] = torch.stack([m.state_dict()[key] for m in models], 0).mean(0)\n",
        "    new_model.load_state_dict(new_state_dict)\n",
        "    return new_model\n",
        "\n",
        "# Cell 11: Evaluation Function\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in loader:\n",
        "            output = model(X_batch)\n",
        "            preds = torch.argmax(output, dim=1)\n",
        "            y_true.extend(y_batch.cpu().numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
        "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
        "        \"f1\": f1_score(y_true, y_pred, zero_division=0)\n",
        "    }\n",
        "\n",
        "# Cell 12: Federated Learning Pipeline\n",
        "fs_methods = [\"chi2\", \"anova\", \"correlation\"]\n",
        "fe_methods = [\"pca\", \"lda\"]\n",
        "k_features = 10\n",
        "results = []\n",
        "\n",
        "for fs_method in fs_methods:\n",
        "    for fe_method in fe_methods:\n",
        "        print(f\"\\nRunning FS: {fs_method.upper()} + FE: {fe_method.upper()}\")\n",
        "        try:\n",
        "            # Split data into 3 clients\n",
        "            client_splits = np.array_split(np.random.permutation(len(X)), 3)\n",
        "            client_datasets = []\n",
        "\n",
        "            for split in client_splits:\n",
        "                X_client = X[split]\n",
        "                Y_client = Y[split]\n",
        "                X_fs, _ = select_features(X_client, Y_client, method=fs_method, k=k_features, feature_names=feature_names)\n",
        "                n_components = 5 if fe_method == \"pca\" else 1  # Adjust based on method\n",
        "                X_fe = extract_features(X_fs, Y_client, method=fe_method, n_components=n_components)\n",
        "                X_train, X_val, y_train, y_val = train_test_split(X_fe, Y_client, test_size=0.2, random_state=42)\n",
        "                train_loader = DataLoader(TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
        "                                                        torch.tensor(y_train, dtype=torch.long)), batch_size=16, shuffle=True)\n",
        "                val_loader = DataLoader(TensorDataset(torch.tensor(X_val, dtype=torch.float32),\n",
        "                                                      torch.tensor(y_val, dtype=torch.long)), batch_size=64, shuffle=False)\n",
        "                client_datasets.append((train_loader, val_loader))\n",
        "\n",
        "            # Train local models\n",
        "            local_models = []\n",
        "            for train_loader, _ in client_datasets:\n",
        "                model = MLP(n_components)  # Use n_components as input_dim\n",
        "                optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "                criterion = nn.CrossEntropyLoss()\n",
        "                train(model, train_loader, criterion, optimizer)\n",
        "                local_models.append(model)\n",
        "\n",
        "            # Aggregate global model\n",
        "            global_model = average_models(local_models)\n",
        "\n",
        "            # Evaluate on combined validation data\n",
        "            combined_X, combined_y = [], []\n",
        "            for _, val_loader in client_datasets:\n",
        "                for X_batch, y_batch in val_loader:\n",
        "                    combined_X.append(X_batch)\n",
        "                    combined_y.append(y_batch)\n",
        "            X_all = torch.cat(combined_X)\n",
        "            y_all = torch.cat(combined_y)\n",
        "            val_loader = DataLoader(TensorDataset(X_all, y_all), batch_size=64)\n",
        "            metrics = evaluate(global_model, val_loader)\n",
        "            results.append({\n",
        "                \"FS\": fs_method.upper(),\n",
        "                \"FE\": fe_method.upper(),\n",
        "                \"Accuracy\": round(metrics['accuracy'], 4),\n",
        "                \"Precision\": round(metrics['precision'], 4),\n",
        "                \"Recall\": round(metrics['recall'], 4),\n",
        "                \"F1 Score\": round(metrics['f1'], 4),\n",
        "            })\n",
        "            print(f\"Completed FS: {fs_method.upper()}, FE: {fe_method.upper()}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error for FS: {fs_method.upper()}, FE: {fe_method.upper()}: {str(e)}\")\n",
        "\n",
        "# Display results\n",
        "print(\"\\nSummary Results Table:\")\n",
        "df_results = pd.DataFrame(results)\n",
        "print(df_results.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TTRVGDnyJItc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}